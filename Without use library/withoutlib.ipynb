{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model, Scaler, and Label Encoders saved successfully.\n",
      "ðŸŽ¯ Model Accuracy (RÂ² Score): 99.95%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "    headers = data[0]\n",
    "    return headers, data[1:]\n",
    "\n",
    "# Encode categorical variables manually\n",
    "def label_encode_column(column):\n",
    "    unique_values = list(set(column))\n",
    "    encoding = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    return [encoding[val] for val in column], encoding\n",
    "\n",
    "# Standardize numerical features\n",
    "def standardize_features(X):\n",
    "    X = np.array(X, dtype=float)\n",
    "    means = np.mean(X, axis=0)\n",
    "    stds = np.std(X, axis=0)\n",
    "    return (X - means) / stds, means, stds\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split(X, y, test_size=0.3):\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    return X[:split_idx], X[split_idx:], y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Custom Decision Tree Regressor\n",
    "class DecisionTreeRegressorCustom:\n",
    "    def __init__(self, min_samples_split=2):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        if len(y) < self.min_samples_split or len(set(y)) == 1:\n",
    "            return np.mean(y)\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.mean(y)\n",
    "        left_idx = X[:, best_feature] <= best_threshold\n",
    "        right_idx = X[:, best_feature] > best_threshold\n",
    "        left_subtree = self._build_tree(X[left_idx], y[left_idx])\n",
    "        right_subtree = self._build_tree(X[right_idx], y[right_idx])\n",
    "        return (best_feature, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_feature, best_threshold, best_variance = None, None, float('inf')\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_idx = X[:, feature] <= threshold\n",
    "                right_idx = X[:, feature] > threshold\n",
    "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
    "                    continue\n",
    "                variance = np.var(y[left_idx]) * len(y[left_idx]) + np.var(y[right_idx]) * len(y[right_idx])\n",
    "                if variance < best_variance:\n",
    "                    best_feature, best_threshold, best_variance = feature, threshold, variance\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def predict_one(self, x, node):\n",
    "        if not isinstance(node, tuple):\n",
    "            return node\n",
    "        feature, threshold, left, right = node\n",
    "        if x[feature] <= threshold:\n",
    "            return self.predict_one(x, left)\n",
    "        else:\n",
    "            return self.predict_one(x, right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_one(x, self.tree) for x in X])\n",
    "\n",
    "# Load and process dataset\n",
    "headers, data = load_csv('Housing_Data.csv')\n",
    "data = np.array(data)\n",
    "\n",
    "# Separate features and target\n",
    "y = data[:, headers.index('price')].astype(float)\n",
    "X = np.delete(data, headers.index('price'), axis=1)\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for i in range(X.shape[1]):\n",
    "    if not X[:, i][0].replace('.', '', 1).isdigit():\n",
    "        X[:, i], label_encoders[headers[i]] = label_encode_column(X[:, i])\n",
    "X = X.astype(float)\n",
    "\n",
    "# Standardize features\n",
    "X_scaled, means, stds = standardize_features(X)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
    "\n",
    "# Train model\n",
    "model = DecisionTreeRegressorCustom()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "\n",
    "# Save model and preprocessing objects\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump({'means': means, 'stds': stds}, f)\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "print(\"âœ… Model, Scaler, and Label Encoders saved successfully.\")\n",
    "print(f\"ðŸŽ¯ Model Accuracy (RÂ² Score): {r2 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
